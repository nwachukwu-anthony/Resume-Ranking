{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Ranking\n",
    "\n",
    "___ \n",
    "We try to rank resume profiles based on content similarity. The BM25 family of ranking models were considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textract\n",
    "# !pip install tika"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import textract\n",
    "from tika import parser\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths to inputs and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./../Data/Resumes/\"\n",
    "save_to_path = \"./../Data/Workin_Data/\"\n",
    "\n",
    "for filename in glob.glob(path+\"~*\"):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility class that serves as helper class containing functions regulary used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "\n",
    "\n",
    "class Utils:\n",
    "    \"\"\"\n",
    "    Class containing methods that serve as helper functions\n",
    "    \"\"\"\n",
    "    \n",
    "    def shuffle_data(self, data_pd):\n",
    "        \"\"\"\n",
    "        Data shuffling\n",
    "        \"\"\"\n",
    "        \n",
    "        data_columns = data_pd.columns\n",
    "        data_body = data_pd[data_columns]\n",
    "        data_body = shuffle(data_body)\n",
    "\n",
    "        return data_body\n",
    "    \n",
    "    def string_to_words(self, query):\n",
    "        \"\"\"\n",
    "        from string of words to list of processed words\n",
    "        \"\"\"\n",
    "        \n",
    "        nltk.download(\"stopwords\", quiet=True)\n",
    "        try:\n",
    "            # add_similar_words_to_search_query(query[-1])\n",
    "            text = BeautifulSoup(query[-1], \"html.parser\").get_text()  # Remove HTML tags\n",
    "            text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())  # Remove non-alphanumeric and Convert to lower case\n",
    "        except:\n",
    "            text = ''\n",
    "        word_list = text.split()  # Split string into words\n",
    "        word_list = [w for w in word_list if w not in stopwords.words(\"english\")]  # Remove stopwords\n",
    "        word_list = [PorterStemmer().stem(w) for w in word_list]  # stem\n",
    "\n",
    "        return [query[0], word_list]\n",
    "    \n",
    "    def clean_data(self, data, cache_dir, cache_file=\"cleaned_data.pkl\"):\n",
    "        \"\"\"\n",
    "        Convert each data row to words; read from cache if available.\n",
    "        input: dataframe with columns key->col1, value->col2\n",
    "        output: list of lists, e.g [[employee1_id,body1_word_list],[employee2_id,body2_word_list],...]\n",
    "        \"\"\"\n",
    "\n",
    "        data_keys, data_body = data[data.columns[0]].values, data[data.columns[1]].values\n",
    "        data_train = [[data_keys[i], data_body[i]] for i in range(len(data_body))]\n",
    "\n",
    "        # If cache_file is not None, try to read from it first\n",
    "        cache_data = None\n",
    "        if cache_file is not None:\n",
    "            try:\n",
    "                with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                    cache_data = pickle.load(f)\n",
    "                print(\"Read cleaned data from cache file:\", cache_file)\n",
    "            except:\n",
    "                pass  # unable to read from cache, but that's okay\n",
    "\n",
    "        # If cache is missing, then do the heavy lifting\n",
    "        if cache_data is None:\n",
    "            # Preprocess the data to obtain words for each employee data\n",
    "            words_train = list(map(self.string_to_words, data_train))\n",
    "\n",
    "            # Write to cache file for future runs\n",
    "            if cache_file is not None:\n",
    "                cache_data = dict(words_train=words_train)\n",
    "                with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                    pickle.dump(cache_data, f)\n",
    "                print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "        else:\n",
    "            # Unpack data loaded from cache file\n",
    "            words_train = (cache_data['words_train'])\n",
    "\n",
    "        return words_train\n",
    "    \n",
    "    def add_data_to_pickle(self, data_file, data=None, path='./'):\n",
    "\n",
    "        data_path = path + data_file\n",
    "\n",
    "        data_file_name = data_file.split('.')[0]\n",
    "        pickle_file_name = path + 'data_dict.pkl'\n",
    "\n",
    "        if os.path.isfile(pickle_file_name):\n",
    "            pickle_file = open(pickle_file_name, 'rb')\n",
    "        else:\n",
    "            pickle_file = open(pickle_file_name, 'bw')\n",
    "            pickle_file.close()\n",
    "\n",
    "        if os.path.getsize(pickle_file_name) > 0:\n",
    "            data_collections = pickle.load(pickle_file)\n",
    "            pickle_file.close()\n",
    "        else:\n",
    "            data_collections = {}\n",
    "\n",
    "\n",
    "        data_collections[data_file_name] = data\n",
    "        with open(pickle_file_name, 'bw') as f:\n",
    "            pickle.dump(data_collections, f)\n",
    "            \n",
    "    def swap_key_value(self, index_id):\n",
    "        return {emm_id: index for index, emm_id in index_id.items()}\n",
    "    \n",
    "    def resume_index_id_data(self, data_processed_with_id):\n",
    "        count, index_id, data = 0, {}, []\n",
    "        for item in data_processed_with_id:\n",
    "            index_id[count] = item[0]\n",
    "            data.append(item[1])\n",
    "            count += 1\n",
    "        return index_id, data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class containing functions that helps to extract data from the document resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecruitmentPreprocess:\n",
    "    \"\"\"\n",
    "    Class for preprocessing job offer and resume text queries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resume_path, save_to_path=None):\n",
    "        \n",
    "        self.resume_path = resume_path\n",
    "        \n",
    "        if save_to_path != None:\n",
    "            self.save_to_path = save_to_path\n",
    "            self.resume_id_index, self.resume_id_data, self.resume_data = self.__merge_resume_to_dataframe()\n",
    "        \n",
    "\n",
    "    def extract_text_from_resume(self, file_name):\n",
    "        if file_name.split('.')[-1] == \"pdf\":\n",
    "            text = parser.from_file(self.resume_path + file_name)['content']\n",
    "        else:\n",
    "            text = textract.process(self.resume_path + file_name).decode()\n",
    "        return text\n",
    "\n",
    "\n",
    "    def __merge_resume_to_dataframe(self):\n",
    "\n",
    "        all_collection = []\n",
    "        all_files = [file.split('\\\\')[-1] for file in glob.glob(path + \"*\") if not file.startswith('~')]\n",
    "        index = 1\n",
    "        resume_id_index = {}\n",
    "        resume_id_data = {}\n",
    "        \n",
    "        for file in all_files:\n",
    "            if file.split('.')[-1] in ['docx', 'pdf', 'doc']:\n",
    "                resume_id_index[index] = '{}_{}'.format(''.join(file.split('.')[:-1]),index)\n",
    "                resume_id_data[index] = self.extract_text_from_resume(file)\n",
    "                collection = [index, self.extract_text_from_resume(file)]\n",
    "                all_collection.append(collection)\n",
    "                index += 1\n",
    "\n",
    "        return resume_id_index, resume_id_data, pd.DataFrame(all_collection, columns=['employee_id', 'data'])\n",
    "    \n",
    "    def save_resume_data_to_csv(self):\n",
    "        \n",
    "        self.resume_data.to_csv(self.save_to_path + 'resume_data.csv', index=False)\n",
    "        \n",
    "    def add_resume_keys_to_pickle(self):\n",
    "        \n",
    "        utils = Utils()\n",
    "        utils.add_data_to_pickle('resume_id_index', self.resume_id_index, self.save_to_path)\n",
    "        \n",
    "    def add_user_accessible_resume_to_pickle(self):\n",
    "        \n",
    "        utils = Utils()\n",
    "        utils.add_data_to_pickle('user_accessible_resume', self.resume_id_data, self.save_to_path)\n",
    "        \n",
    "    def add_processed_resume_to_pickle(self):\n",
    "        \n",
    "        cache_directory = os.path.join(\"cache\", \"words_tokens\")  # where to store cache files\n",
    "        os.makedirs(cache_directory, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "        cache_file = 'cleaned_{}.pkl'.format('processed_resume')\n",
    "        \n",
    "        utils = Utils()\n",
    "    \n",
    "        data_shuffled = utils.shuffle_data(self.resume_data)\n",
    "        data_processed_with_id = utils.clean_data(data_shuffled, cache_directory, cache_file=cache_file)\n",
    "        \n",
    "        shutil.rmtree('cache')\n",
    "        \n",
    "        utils.add_data_to_pickle('processed_resume', data_processed_with_id, self.save_to_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the resume files and storing them in a pickle file for faster access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: cleaned_processed_resume.pkl\n"
     ]
    }
   ],
   "source": [
    "resume = RecruitmentPreprocess(path, save_to_path)\n",
    "resume.save_resume_data_to_csv()\n",
    "resume.add_resume_keys_to_pickle()\n",
    "resume.add_user_accessible_resume_to_pickle()\n",
    "resume.add_processed_resume_to_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming that the files were extracted successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['resume_id_index', 'user_accessible_resume', 'processed_resume'])\n",
      "Bharatha BA Resume_31\n",
      "Reddemma Lankipalle\n",
      "\n",
      "Software Test Engineer - Crozer-Keystone Health System\n",
      "\n",
      "\n",
      "\n",
      "Norristown, PA 19403\n",
      "\n",
      "\n",
      "\n",
      "reddemmaqa2_hr3@indeedemail.com - 4846862294\n",
      "\n",
      "\n",
      "\n",
      "Having 8 years of experience in Software testing in the areas of Web based, Client-Server applications using Manual and Automation testing techniques. Well versed in Manual testing techniques and methodologies.\n",
      "\n",
      "\n",
      "\n",
      "Summary\n",
      "\n",
      "\n",
      "\n",
      "\tHands on experience in Quality Assurance including Functional, System, Smoke, GUI, Regression, and Integration, Retesting, User Acceptance Test (UAT) and Compatibility Testing of Web and Client server based Applications.\n",
      "\n",
      "\n",
      "\n",
      "\tExperience in both Agile and Waterfall Software Development Models.\n",
      "\n",
      "\n",
      "\n",
      "\tExtensive experience in designing Test Cases, Test Scenarios, Test Scripts and Test reports of manual and automated tests.\n",
      "\n",
      "\n",
      "\n",
      "\tExtensive experience in coordinating testing effort, responsible for test deliverables, status reporting to management and issue escalations.\n",
      "\n",
      "\n",
      "\n",
      "\tExperienced in complete Software Development Life Cycle (SDLC), Software Testing Life Cycle (STLC) and Bug Life Cycles\n",
      "\n",
      "\n",
      "\n",
      "\tExpertise on Black-Box testing techniques like BVA and ECP.\n",
      "\n",
      "\n",
      "\n",
      "\tExperienced in creating Requirement Traceability Matrix (RTM) to ensure comprehensive test coverage of requirements.\n",
      "\n",
      "\n",
      "\n",
      "\tExperienced in creating Test Summary Report for each sprint.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in Testing in Agile Development Environment using TDD and ATDD approach.\n",
      "\n",
      "\n",
      "\n",
      "\tExperience in effectively coordinating with development team for any clarification on defects or reproducing the defects.\n",
      "\n",
      "\n",
      "\n",
      "\tExperienced in UI Automation Testing for Android/iOS Mobile Application using Appium.\n",
      "\n",
      "\n",
      "\n",
      "\tDeveloped Appium Scripts for Native Android/iOS Apps using Java.\n",
      "\n",
      "\n",
      "\n",
      "\tActively involved in peer review meetings, requirement analysis, defect reviews with the development team and training the team members\n",
      "\n",
      "\n",
      "\n",
      "\tWorking experience with defect tracking tools like Jira, Quality Center (QC) and Microsoft Test Manager (MTM).\n",
      "\n",
      "\n",
      "\n",
      "\tGood experience in database testing (SQL Server/Oracle) and SQL queries.\n",
      "\n",
      "\n",
      "\n",
      "\tExperience in web applications testing with Selenium IDE, Selenium RC, Selenium Grid and Selenium Web Driver, JUnit and TestNG, Maven, ANT, Jenkins.\n",
      "\n",
      "\n",
      "\n",
      "\tDeveloped framework using Cucumber, Junit, TestNG, Page Object Model, and Page Factory\n",
      "\n",
      "\n",
      "\n",
      "\tDeveloped BDD tests using Cucumber by writing behavior and step definitions & developed required Selenium support code in JAVA for Cucumber.\n",
      "\n",
      "\n",
      "\n",
      "\tGood experience in testing both SOAP and REST services using SOAP UI and Tested both XML and JSON formats.\n",
      "\n",
      "\n",
      "\n",
      "\tExcellent experience with source version control tools such as Subversion (SVN), TFS & Git.\n",
      "\n",
      "\n",
      "\n",
      "\tPerformed Automated Database testing using Selenium Web Driver and Microsoft JDBC API.\n",
      "\n",
      "\n",
      "\n",
      "\tExperience in doing performance testing of service using Headless Browser and JMeter.\n",
      "\n",
      "\n",
      "\n",
      "\tExcellent experience with Protractor, Java scripts to automate Angular JS application.\n",
      "\n",
      "\n",
      "\n",
      "\tExtensive experience in Functional Automation Testing tool Quick Test Professional ( QTP )\n",
      "\n",
      "\n",
      "\n",
      "\tExperienced in Test coordination between Onsite and Offshore team.\n",
      "\n",
      "\n",
      "\n",
      "\tWorked collaboratively and cross-functionally as part of a team as well as an independently.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tAbility to interact with end users, also quick learner and team player with good communication skills.\n",
      "\n",
      "\n",
      "\n",
      "\tExcellent analytical and problem solving skills with the ability to troubleshoot and strong oral and written communication skills.\n",
      "\n",
      "\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "\n",
      "\n",
      "Software Test Engineer\n",
      "\n",
      "\n",
      "\n",
      "\t\tCrozer-Keystone Health System\t-\tSpringfield, PA -\n",
      "\n",
      "\n",
      "\n",
      "2017-01 - Present\n",
      "\n",
      "\n",
      "\n",
      "Description: The job of the TPA is to maintain databases of policyholders and issue them identity cards with unique identification numbers and handle all the post policy issues including claim settlements.\n",
      "\n",
      "\n",
      "\n",
      "Third party administrators are prominent players in the Managed Care industry and have the expertise and capability to administer all or a portion of the claims process. They are normally contracted by a health insurer or self-insuring companies to administer services, including claims administration, premium collection, enrollment and other administrative activities. A hospital or provider organization desiring to set up its own health plan will often outsource certain responsibilities to a TPA.\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities:\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in creating Test Scenarios and Test Cases based on Use Cases and Business Requirements.\n",
      "\n",
      "\n",
      "\n",
      "\tTested application utilizing the Agile methodology and worked on multiple sprints on the project\n",
      "\n",
      "\n",
      "\n",
      "\tTested in Development, Staging and Production Environments.\n",
      "\n",
      "\n",
      "\n",
      "\tPrepared Test Summary Report.\n",
      "\n",
      "\n",
      "\n",
      "\tExecuted Tests, and evaluated Test results\n",
      "\n",
      "\n",
      "\n",
      "\tUsed Jira for bug tracking and reporting, also followed up with development team to verify bug fixes, and update bug status, Manage requirements and maintained test repository.\n",
      "\n",
      "\tHelped new team members to understand the Business requirement and system flow of an application\n",
      "\n",
      "\n",
      "\n",
      "\tWorking with the SQL statements to extract data from Oracle tables and verify the output data.\n",
      "\n",
      "\n",
      "\n",
      "\tUsed emulators and IOS simulators to perform the Mobile Testing.\n",
      "\n",
      "\n",
      "\n",
      "\tValidated the both Request and Response messages for REST Web Service.\n",
      "\n",
      "\n",
      "\n",
      "\tActively involved in daily \"Scrum meeting\" and provide status report.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in design and implementation of Selenium WebDriver automation framework for smoke and regression test suites (TestNG, Maven).\n",
      "\n",
      "\tConducted the team meetings for discussing the status and issues.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in writing automated scripts for angular JS application using protractor and Java scripts.\n",
      "\n",
      "\n",
      "\n",
      "\tLead a small team off shore and onshore, prioritize tasks and deliver.\n",
      "\n",
      "\n",
      "\n",
      "\tPerformed end-to-end testing of application and also verified compliance for software quality standards of the organization.\n",
      "\n",
      "\tDeveloped reusable functions and maintained in the function libraries.\n",
      "\n",
      "\n",
      "\n",
      "\tGood Experience in Agile methodology with SCRUM Process for the product development.\n",
      "\n",
      "\n",
      "\n",
      "\tUsed Selenium Grid to run test cases in multiple browsers and Platforms.\n",
      "\n",
      "\n",
      "\n",
      "\tDesign and developing the automation framework (Page Object Model Framework) for web and mobile application automation using selenium and Appium.\n",
      "\n",
      "\tExperienced in load and performance testing tool JMeter.\n",
      "\n",
      "\n",
      "\n",
      "\tDeployment of jobs on individual stages on Jenkins, and running automated and regression tests.\n",
      "\n",
      "\n",
      "\n",
      "Environment: Java, Selenium, Jira, Git, Oracle, MS Office, HTML, Windows XP/Vista, Appium (IOS/Android), Eclipse, Soap UI, Jmeter, Cucumber, Agile Process.\n",
      "\n",
      "\n",
      "\n",
      "Software Test Engineer\n",
      "\n",
      "\n",
      "\n",
      "\t\tInQuicker\t-\tNashville, TN -\n",
      "\n",
      "\n",
      "\n",
      "2015-07 - 2016-12\n",
      "\n",
      "\n",
      "\n",
      "Description: This is a web based application provides different features to Customers and Employees of the Organization. Ones The Client tie-up with this organization the organization will give user name and password, if he gets and go through the portal the customer should be submit user name and password then after, that application is worth full to him, then he can access anywhere to this application. Front-end application entirely shows the customer medical reports and as well as Cap questioner that is client medical details, After customer finish the CAP questioner the Vidal organization employee's will handle the rest of thing issues of client's. Vidal Application has a set of features commonly deployed using two types of views,\n",
      "\n",
      "\n",
      "\n",
      "\tFront-End View (This is entirely for Client or customer, he only use this application) the Standard tabs for application (Login, Fitness, Diet, Illness, Stress, Lifestyle)\n",
      "\n",
      "\tBack-End view (This is entirely for organization employees, they only use this application) the Standard tabs for application (Login, Manage corporate, Data entry, Reports (Client Information, Manage password)). Responsibilities:\n",
      "\n",
      "\n",
      "\n",
      "• Involved in designing of Automation Test cases using Selenium WebDriver, Java, TestNG, and Maven.\n",
      "\n",
      "\n",
      "\n",
      "• Used Selenium Grid to run test cases in multiple browsers and Platforms.\n",
      "\n",
      "\n",
      "\n",
      "• Involved in implementation of Hybrid Test Automation Framework, Page Object Model using Selenium WebDriver, TestNG and Maven and Java.\n",
      "\n",
      "• Integrated Automation scripts (Selenium WebDriver API) in Continuous Integration tools (Jenkins) for nightly batch run of the Script.\n",
      "\n",
      "• Logged and managed defects using Microsoft Test Manager (MTM) defect management tool.\n",
      "\n",
      "\n",
      "\n",
      "• Developed automation script for Mobile Native applications (IOS, Android) using Appium, Java.\n",
      "\n",
      "\n",
      "\n",
      "• Used emulators and IOS simulators to perform the manual Mobile Testing.\n",
      "\n",
      "\n",
      "\n",
      "• Involved in preparing the functional test cases using Test design techniques data validation concepts Equivalence partitioning, boundary value analysis.\n",
      "\n",
      "• Strong experience with SOA architecture which include Soap and Restful Web Services Testing.\n",
      "\n",
      "\n",
      "\n",
      "• Performed web services testing with SOAP UI by validating request and response for XML, JSon files.\n",
      "\n",
      "\n",
      "\n",
      "• Involved in testing of services using SOAP/REST services using SOAP UI, Groovy Script.\n",
      "\n",
      "\n",
      "\n",
      "• Performed data driven testing by using JDBC and Groovy script as a data source in SOAP UI and configured SQL queries to fetch data from the Oracle database.\n",
      "\n",
      "• Involved in writing automated scripts for angular JS application using protractor and Java scripts.\n",
      "\n",
      "\n",
      "\n",
      "• Developed Selenium scripts in TestNG for parameterization using Data Provider annotation.\n",
      "\n",
      "\n",
      "\n",
      "• Involved in Maven configuration for running servers and scripts after the build.\n",
      "\n",
      "\n",
      "\n",
      "• Prepared manual & function test cases on web and mobile applications (Apple & Android) as per requirements.\n",
      "\n",
      "• Used Selenium Grid to run test cases in multiple browsers and Platforms.\n",
      "\n",
      "\n",
      "\n",
      "• Used Jenkins as continuous integration server to run automated test suites.\n",
      "\n",
      "\n",
      "\n",
      "• Used web-debugging tools like XPath, Firebug and Fire path to locate elements.\n",
      "\n",
      "\n",
      "\n",
      "Environment: Selenium WebDriver, Jenkins, Cucumber, SOAP UI, Maven, Appium (IOS/Android), TestNG, Jmeter, Eclipse, XPATH, Java, Android, SQL Server, JMeter, Protractor, SVN, Test Manager, Selenium Grid.\n",
      "\n",
      "\n",
      "\n",
      "Software Test Engineer\n",
      "\n",
      "\n",
      "\n",
      "\t\tAlliant Healthcare Products\t-\tRichland, MI -\n",
      "\n",
      "\n",
      "\n",
      "2014-06 - 2015-07\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Description: Alliant Healthcare Products is a division of Alliant Enterprises LLC, established in 2002, to provide\n",
      "\n",
      "\n",
      "\n",
      "quality, cost-effective medical device products and services. The company's owner and CEO is Bob Taylor,\n",
      "\n",
      "\n",
      "\n",
      "a U.S. Air Force and Air Force Reserve veteran with a track record of creating value for customers and\n",
      "\n",
      "\n",
      "\n",
      "partners in the medical device industry. Today, many of Alliant Healthcare's employees are veterans, and the\n",
      "\n",
      "\n",
      "\n",
      "company remains committed to supporting the veteran community. With a core focus on the federal market,\n",
      "\n",
      "\n",
      "\n",
      "Alliant Healthcare is not your average SDVOSB. Alliant employs an extensive team of government contracting\n",
      "\n",
      "\n",
      "\n",
      "experts, customer service representatives, field sales representatives, and many more talented individuals.\n",
      "\n",
      "\n",
      "\n",
      "With resources that surpass other SDVOSB's, Alliant is able to make sure government purchases receive the\n",
      "\n",
      "\n",
      "\n",
      "care and attention they deserve.\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities:\n",
      "\n",
      "\n",
      "\n",
      "\tProject is developed using agile methodology.\n",
      "\n",
      "\n",
      "\n",
      "\tPerformed Positive and Negative testing by inputting valid and invalid data\n",
      "\n",
      "\n",
      "\n",
      "\tParticipated in daily Scrum meetings to discuss the status of work done.\n",
      "\n",
      "\n",
      "\n",
      "\tUsed Selenium Grid to run test cases in multiple browsers and Platforms.\n",
      "\n",
      "\n",
      "\n",
      "\tPerformed Regression testing to ensure that changes made to software did not introduce any new bugs.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in creating Requirement Traceability Matrix (RTM).\n",
      "\n",
      "\n",
      "\n",
      "\tImplemented Try Catch Block to handle unexpected events or errors.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in designing of Automation Test cases using Selenium WebDriver, Java, Junit, and ANT.\n",
      "\n",
      "\n",
      "\n",
      "\tLogged and managed defects using Rally defect management tool.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in Backend Database testing by using Sql Developer Tool.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in testing of services using SOAP/REST services using SOAP UI, Groovy Script.\n",
      "\n",
      "\n",
      "\n",
      "\tBuilt automation test framework for client side products using Selenium WebDriver.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in Knowledge Transfer, Training Session and Brainstorming.\n",
      "\n",
      "\n",
      "\n",
      "\tPrepared the Test Results document which summarizes testing activities and results.\n",
      "\n",
      "\n",
      "\n",
      "Environment: Selenium, SOAPUI, Rally, Ruby, Appium (IOS), ASP.NET, Oracle, UNIX, MS Office, HTML, Windows XP/Vista, Git, Selenium Grid.\n",
      "\n",
      "\n",
      "\n",
      "QA Engineer\n",
      "\n",
      "\n",
      "\n",
      "\t\tCommunication Infrastructure Corporation\t-\tOxnard, CA -\n",
      "\n",
      "\n",
      "\n",
      "2013-01 - 2014-05\n",
      "\n",
      "\n",
      "\n",
      "Description: Communication Infrastructure Corporation (CIC) is an industry leader and single source provider\n",
      "\n",
      "\n",
      "\n",
      "for microwave and point to multipoint radio networks. The CIC team has over 1,000 years of collective\n",
      "\n",
      "\n",
      "\n",
      "experience in applications for wireless backhaul, ultra-low latency, oil & gas, rural broadband, mobility, and\n",
      "\n",
      "\n",
      "\n",
      "wireless communication links.\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities:\n",
      "\n",
      "\n",
      "\n",
      "\tReviewed Business Requirement and Software Specification Requirements to achieve better understanding of the AUT.\n",
      "\n",
      "\tAnalyzed and reviewed Story Boards to gain overall understanding of the functionality of the application.\n",
      "\n",
      "\n",
      "\n",
      "\tDeveloped automated test scripts from manual test cases for Regression testing based on the requirement documents using Quick Test Professional.\n",
      "\n",
      "\tDeveloped and executed test cases integration, system and regression testing throughout the software development life cycle.\n",
      "\n",
      "\tImplemented Recovery Scenario Manager to handle unexpected events or errors.\n",
      "\n",
      "\n",
      "\n",
      "\tDefects were Tracked, Reviewed, Analyzed and Compared using Quality Center.\n",
      "\n",
      "\n",
      "\n",
      "\tParticipated in Test Case review and Test case sign off meetings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "• Tested in Development, Staging and for Production Environments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tPerformed end-to-end testing of application and also verified compliance for software quality standards of the organization.\n",
      "\n",
      "\tPerformed Automated Regression tests for every modified build.\n",
      "\n",
      "\n",
      "\n",
      "\tConducted weekly meetings with the developers and build Team.\n",
      "\n",
      "\n",
      "\n",
      "Environment: QTP 9, Quality center, Windows, Linux, Java, Tomcat, XML, VBScript, HTML, Oracle and TFS.\n",
      "\n",
      "\n",
      "\n",
      "QA Engineer\n",
      "\n",
      "\n",
      "\n",
      "\t\tVidal Healthcare\t-\tBengaluru, Karnataka -\n",
      "\n",
      "\n",
      "\n",
      "2011-01 - 2012-12\n",
      "\n",
      "\n",
      "\n",
      "Description: Our dedication to payment technology and industry innovation is consistent; our mission is clear:\n",
      "\n",
      "\n",
      "\n",
      "to make it simple for business owners to collect money. Whether business owners accepts credit cards at\n",
      "\n",
      "\n",
      "\n",
      "a retail storefront, processes payments on a mobile phone, promote gift cards, operates an online shopping\n",
      "\n",
      "\n",
      "\n",
      "cart, or electronically invoice clients, Pays cape offers financial products and services guaranteed to satisfy\n",
      "\n",
      "\n",
      "\n",
      "cash flow needs.\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities:\n",
      "\n",
      "\n",
      "\n",
      "\tDeveloped Test Cases, Test Scripts, Test Scenarios, Test Data and Traceability Matrix.\n",
      "\n",
      "\n",
      "\n",
      "\tDeveloped and Implemented Test suites, which include features like Data Driven Framework for automated scripts using QTP.\n",
      "\n",
      "\tPerformed functional, regression, smoke, and end-end testing.\n",
      "\n",
      "\n",
      "\n",
      "\tGenerated automated scripts using QTP and documented them. Created and maintained functional & regression test suites.\n",
      "\n",
      "\tPlayed a key role in script enhancement process to streamline the regression testing process and to achieve continuous automatic test run.\n",
      "\n",
      "\tPerformed Mobile Testing and Automated Mobile Simulator using QTP.\n",
      "\n",
      "\n",
      "\n",
      "\tExperience in creation of library functions, Checkpoints, Data Validation Descriptive Programming using QTP.\n",
      "\n",
      "\tDeveloped and executed SQL Queries to perform database testing Environment.\n",
      "\n",
      "\n",
      "\n",
      "\tDesigned and documented test cases derived from requirements.\n",
      "\n",
      "\n",
      "\n",
      "\tManually verified the applications according to the requirements. Modified automation scripts as test case changes.\n",
      "\n",
      "\tAnalyzed the specifications in the User Requirements Document, Functional Requirement\n",
      "\n",
      "\n",
      "\n",
      "\tParticipated in Weekly meetings and walk through to discuss Defect Priorities.\n",
      "\n",
      "\n",
      "\n",
      "Environment: HTML, SQL Server 2005, and Windows 2003, Testing Tools: QTP 8.2, Mercury Quality Center 9, VBScript, and SVN.\n",
      "\n",
      "\n",
      "\n",
      "QA Tester\n",
      "\n",
      "\n",
      "\n",
      "\t\tHSBC\t-\tBengaluru, Karnataka -\n",
      "\n",
      "\n",
      "\n",
      "2009-10 - 2010-12\n",
      "\n",
      "\n",
      "\n",
      "HSBC is one of the world's largest banking and financial services organizations. With around 6,600 offices in both established and faster-growing markets, we aim to be where the growth is, connecting customers to opportunities, enabling businesses to thrive and economies to prosper and, ultimately, helping people to fulfil their hopes and realize their ambitions. We serve around 58 million customers through our four global businesses: Retail Banking and Wealth Management, Commercial Banking, Global Banking and Markets, and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Global Private Banking. Our network covers 80 countries and territories in Europe, the Asia-Pacific region, the\n",
      "\n",
      "\n",
      "\n",
      "Middle East, Africa, North America and Latin America. Our aim is to be acknowledged as the world's leading\n",
      "\n",
      "\n",
      "\n",
      "international bank.\n",
      "\n",
      "\n",
      "\n",
      "Responsibilities:\n",
      "\n",
      "\n",
      "\n",
      "\tProject is developed using Waterfall methodology.\n",
      "\n",
      "\n",
      "\n",
      "\tUnderstanding the customer requirements and functionality of the application.\n",
      "\n",
      "\n",
      "\n",
      "\tIdentifying Test Scenarios by using Use cases.\n",
      "\n",
      "\n",
      "\n",
      "\tInvolved in Test Data preparation and Test execution\n",
      "\n",
      "\n",
      "\n",
      "\tAnalyzing the impacts of new CRs on existing scenarios and updating the Test cases.\n",
      "\n",
      "\n",
      "\n",
      "\tConducting various tests such as Smoke testing, GUI, functional and retesting, regression testing, user interface testing etc.\n",
      "\n",
      "\tInvolved in creating the Requirement Traceability Matrix (RTM).\n",
      "\n",
      "\n",
      "\n",
      "\tPrepared Test Summary Report.\n",
      "\n",
      "\n",
      "\n",
      "\tAnalyzing the Results.\n",
      "\n",
      "\n",
      "\n",
      "\tExecuted SQL queries for data verification to compare the expected results with database.\n",
      "\n",
      "\n",
      "\n",
      "Environment: Bugzilla, Oracle9i, Java, Toad, IE, Mozilla, Chrome Browser.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "\n",
      "\n",
      "Bachelors of Technology in Technology\n",
      "\n",
      "\n",
      "\n",
      "SVU University\n",
      "\n",
      "\n",
      "\n",
      "2007\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SKILLS\n",
      "\n",
      "\n",
      "\n",
      "TESTING, SQL, JAVA, HTML, ORACLE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "\n",
      "\n",
      "TECHNICAL SKILLS\n",
      "\n",
      "\n",
      "\n",
      "Testing Tools Selenium 2 Web Driver/RC/IDE/Grid, Quick Test Pro (QTP/UFT), Soap UI,Protractor, JMeter, Appium (iOS/Android), Jasmine, Cucumber\n",
      "\n",
      "\n",
      "\n",
      "Defect Tracking Tools Rally, Quality Center (9, 10, 11.0), Jira, Bugzilla, Microsoft Test Manager (MTM) Browsers Internet Explorer, Firefox, Chrome, Safari Programming Languages Java, Ruby, SQL, PL/SQL\n",
      "\n",
      "\n",
      "\n",
      "Web Services SOA, REST, SOAP UI\n",
      "\n",
      "\n",
      "\n",
      "Web Technologies HTML, Angular JS, AJAX, CSS, Java Script, XML, India Insure web service, SOAP, XSL, XSLT, XHTML, JSON\n",
      "\n",
      "\n",
      "\n",
      "Script Languages Java Script, UNIX Basics, SQL scripting, VB Script Platforms Windows 2003 Server, UNIX, Windows XP/Vista/7/8, MAC OS X Databases Oracle R12, MS SQL Server 2005, 2008 , 2014 Processes\n",
      "\n",
      "\n",
      "\n",
      "Software Testing Life Cycle, Defect Life Cycle, Testing Methodologies, Black Box Testing Techniques, Agile Process, BDD,TDD,ATDD\n",
      "\n",
      "\n",
      "\n",
      "IDE & Reporting Tools Eclipse 3.5,3.7,4.0, NetBeans 6.8\n",
      "\n",
      "\n",
      "\n",
      "Methodologies Agile Scrum, Waterfall\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Build Tools Ant & Maven\n",
      "\n",
      "\n",
      "\n",
      "CI Tools Jenkins\n",
      "\n",
      "\n",
      "\n",
      "Unit Test Frameworks Junit & TestNG\n",
      "\n",
      "\n",
      "\n",
      "Domain Knowledge Health Care, Insurance, Wellness, Banking, ecommerce\n"
     ]
    }
   ],
   "source": [
    "with open(save_to_path+'data_dict.pkl', 'rb') as f:\n",
    "    cv_id_index = pickle.load(f)\n",
    "\n",
    "print(cv_id_index.keys())\n",
    "print(cv_id_index['resume_id_index'][31])\n",
    "print(cv_id_index['user_accessible_resume'][144])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The family of BM25 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, corpus, tokenizer=None):\n",
    "        self.corpus_size = len(corpus)\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if tokenizer:\n",
    "            corpus = self._tokenize_corpus(corpus)\n",
    "\n",
    "        nd = self._initialize(corpus)\n",
    "        self._calc_idf(nd)\n",
    "\n",
    "    def _initialize(self, corpus):\n",
    "        nd = {}  # word -> number of documents with word\n",
    "        num_doc = 0\n",
    "        for document in corpus:\n",
    "            self.doc_len.append(len(document))\n",
    "            num_doc += len(document)\n",
    "\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.doc_freqs.append(frequencies)\n",
    "\n",
    "            for word, freq in frequencies.items():\n",
    "                try:\n",
    "                    nd[word] += 1\n",
    "                except KeyError:\n",
    "                    nd[word] = 1\n",
    "\n",
    "        self.avgdl = num_doc / self.corpus_size\n",
    "        return nd\n",
    "\n",
    "    def _tokenize_corpus(self, corpus):\n",
    "        pool = Pool(cpu_count())\n",
    "        tokenized_corpus = pool.map(self.tokenizer, corpus)\n",
    "        return tokenized_corpus\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_top_n(self, query, documents, n=5):\n",
    "\n",
    "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
    "\n",
    "        scores = self.get_scores(query)\n",
    "        top_n = np.argsort(scores)[::-1][:n]\n",
    "        return [documents[i] for i in top_n]\n",
    "\n",
    "\n",
    "class BM25Okapi(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
    "        # k1=1.5, b=0.75, epsilon=0.25 -> k1=0 to 3, b -> 0 to 1\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        \"\"\"\n",
    "        Calculates frequencies of terms in documents and in corpus.\n",
    "        This algorithm sets a floor on the idf values to eps * average_idf\n",
    "        \"\"\"\n",
    "        # collect idf sum to calculate an average idf for epsilon value\n",
    "        idf_sum = 0\n",
    "        # collect words with negative idf to set them a special epsilon value.\n",
    "        # idf can be negative if word is contained in more than half of documents\n",
    "        negative_idfs = []\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                negative_idfs.append(word)\n",
    "        self.average_idf = idf_sum / len(self.idf)\n",
    "\n",
    "        eps = self.epsilon * self.average_idf\n",
    "        for word in negative_idfs:\n",
    "            self.idf[word] = eps\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        \"\"\"\n",
    "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
    "        this algorithm also adds a floor to the idf value of epsilon.\n",
    "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
    "        :param query:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
    "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
    "        return score\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        \"\"\"\n",
    "        Calculate bm25 scores between query and subset of all docs\n",
    "        \"\"\"\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "        score = np.zeros(len(doc_ids))\n",
    "        doc_len = np.array(self.doc_len)[doc_ids]\n",
    "        for q in query:\n",
    "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
    "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
    "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
    "        return score.tolist()\n",
    "\n",
    "\n",
    "class BM25L(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log(self.corpus_size + 1) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
    "            score += (self.idf.get(q) or 0) * q_freq * (self.k1 + 1) * (ctd + self.delta) / \\\n",
    "                     (self.k1 + ctd + self.delta)\n",
    "        return score\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        \"\"\"\n",
    "        Calculate bm25 scores between query and subset of all docs\n",
    "        \"\"\"\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "        score = np.zeros(len(doc_ids))\n",
    "        doc_len = np.array(self.doc_len)[doc_ids]\n",
    "        for q in query:\n",
    "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
    "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
    "            score += (self.idf.get(q) or 0) * q_freq * (self.k1 + 1) * (ctd + self.delta) / \\\n",
    "                     (self.k1 + ctd + self.delta)\n",
    "        return score.tolist()\n",
    "\n",
    "\n",
    "class BM25Plus(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log((self.corpus_size + 1) / freq)\n",
    "            self.idf[word] = idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
    "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
    "        return score\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        \"\"\"\n",
    "        Calculate bm25 scores between query and subset of all docs\n",
    "        \"\"\"\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "        score = np.zeros(len(doc_ids))\n",
    "        doc_len = np.array(self.doc_len)[doc_ids]\n",
    "        for q in query:\n",
    "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
    "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
    "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
    "        return score.tolist()\n",
    "\n",
    "\n",
    "# BM25Adpt and BM25T are a bit more complicated than the previous algorithms here. Here a term-specific k1\n",
    "# parameter is calculated before scoring is done\n",
    "\n",
    "class BM25Adpt(BM25):\n",
    "    def __init__(self, corpus, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log((self.corpus_size + 1) / freq)\n",
    "            self.idf[word] = idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
    "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
    "        return score\n",
    "\n",
    "\n",
    "\n",
    "class BM25T(BM25):\n",
    "    def __init__(self, corpus, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log((self.corpus_size + 1) / freq)\n",
    "            self.idf[word] = idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
    "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that helps to get the ranking scores from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "class ModelScores:\n",
    "    \"\"\"\n",
    "    Returns Model Scores for single query, single resume and for all resumes \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        with open('./../Data/Workin_Data/'+'data_dict.pkl', 'rb') as f:\n",
    "            self.pickle_data = pickle.load(f)\n",
    "            \n",
    "        self.utils = Utils()\n",
    "    \n",
    "    def get_resume_id_ranking_scores(self, resume_id, model):\n",
    "        \n",
    "        user_accessible_resume = self.pickle_data['user_accessible_resume']\n",
    "        processed_resume = self.pickle_data['processed_resume']\n",
    "\n",
    "        data_index_resume_id, data_processed = self.utils.resume_index_id_data(processed_resume)\n",
    "\n",
    "        # Remove the resume from the list of resumes to be compared to\n",
    "        data_processed_copy = data_processed[:]\n",
    "        index = self.utils.swap_key_value(data_index_resume_id)[resume_id]\n",
    "        del data_processed_copy[index]\n",
    "\n",
    "        raw_query = user_accessible_resume[resume_id]\n",
    "\n",
    "        if model == 'BM25':\n",
    "            chosen_model = BM25(data_processed_copy)\n",
    "        elif model == 'BM25Okapi':\n",
    "            chosen_model = BM25Okapi(data_processed_copy)\n",
    "        elif model == 'BM25L':\n",
    "            chosen_model = BM25L(data_processed_copy)\n",
    "        elif model == 'BM25Adpt':\n",
    "            chosen_model = BM25Adpt(data_processed_copy)\n",
    "        elif model == 'BM25T':\n",
    "            chosen_model = BM25T(data_processed_copy)\n",
    "        else:\n",
    "            chosen_model = BM25Plus(data_processed_copy)\n",
    "\n",
    "        scores = list(chosen_model.get_scores(self.utils.string_to_words(['', raw_query])[-1]))\n",
    "\n",
    "        scores.insert(index, float('inf'))\n",
    "        indices, id_sorted = zip(*sorted(enumerate(scores), reverse=True, key=itemgetter(1)))\n",
    "\n",
    "        return [[data_index_resume_id[indices[i]], id_sorted[i]] for i in range(0, len(indices))][1:]\n",
    "    \n",
    "\n",
    "    def single_query_scores(self, query, model):\n",
    "        \n",
    "        processed_resume = self.pickle_data['processed_resume']\n",
    "        data_index_employee_id, data_processed = self.utils.resume_index_id_data(processed_resume)\n",
    "        \n",
    "        if model == 'BM25':\n",
    "            chosen_model = BM25(data_processed)\n",
    "        elif model == 'BM25Okapi':\n",
    "            chosen_model = BM25Okapi(data_processed)\n",
    "        elif model == 'BM25L':\n",
    "            chosen_model = BM25L(data_processed)\n",
    "        elif model == 'BM25Adpt':\n",
    "            chosen_model = BM25Adpt(data_processed)\n",
    "        elif model == 'BM25T':\n",
    "            chosen_model = BM25T(data_processed)\n",
    "        else:\n",
    "            chosen_model = BM25Plus(data_processed)\n",
    "            \n",
    "        scores = list(chosen_model.get_scores(self.utils.string_to_words(['', query])[-1]))\n",
    "        indices, id_sorted = zip(*sorted(enumerate(scores), reverse=True, key=itemgetter(1)))\n",
    "        \n",
    "        return [[data_index_employee_id[indices[i]], id_sorted[i]] for i in range(len(indices))]\n",
    "    \n",
    "    def single_resume_scores(self, path, file_name, model):\n",
    "        \n",
    "        recruitment_preprocess = RecruitmentPreprocess(path)\n",
    "        query = recruitment_preprocess.extract_text_from_resume(file_name)\n",
    "        \n",
    "        return self.single_query_scores(file_name, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that helps to extract the resume names from the ranked ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class RankResult:\n",
    "    \n",
    "    def __init__(self, no_of_output, model):\n",
    "        \n",
    "        with open(save_to_path+'data_dict.pkl', 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "        self.user_accessible_resume = pickle_data['resume_id_index']\n",
    "        self.no_of_output = no_of_output\n",
    "        self.model_scores = ModelScores()\n",
    "        self.model = model\n",
    "        \n",
    "    def get_ranking_with_resume_id(self, resume_id):\n",
    "        \n",
    "        scores = self.model_scores.get_resume_id_ranking_scores(145, self.model)\n",
    "        ranked_Resume_names = []\n",
    "        \n",
    "        for i in range(self.no_of_output):\n",
    "            if i >= len(scores):\n",
    "                break\n",
    "            ranked_Resume_names.append(self.user_accessible_resume[scores[i][0]])\n",
    "        \n",
    "        return ranked_Resume_names\n",
    "    \n",
    "    def get_ranking_with_query(self, query):\n",
    "        \n",
    "        scores = self.model_scores.single_query_scores(query, self.model)\n",
    "        ranked_Resume_names = []\n",
    "        \n",
    "        for i in range(self.no_of_output):\n",
    "            if i >= len(scores):\n",
    "                break\n",
    "            ranked_Resume_names.append(self.user_accessible_resume[scores[i][0]])\n",
    "        \n",
    "        return ranked_Resume_names\n",
    "    \n",
    "    def get_ranking_with_resume_filename(self, path, file_name):\n",
    "        \n",
    "        scores = self.model_scores.single_resume_scores(path, file_name, self.model)\n",
    "        ranked_Resume_names = []\n",
    "        \n",
    "        for i in range(self.no_of_output):\n",
    "            if i >= len(scores):\n",
    "                break\n",
    "            ranked_Resume_names.append(self.user_accessible_resume[scores[i][0]])\n",
    "        \n",
    "        return ranked_Resume_names\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "\n",
    "1. Get the resume ranking given resume id\n",
    "2. Get the resume ranking given raw query\n",
    "3. Get the resume ranking given file path and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUNITHA Project Manager (1)_191', 'Srivatsan_Project_Manager_187', 'Sahas BA Resume_154', 'AjayKumar_5', 'Syed_Zia_Ashraf_192'] \n",
      "\n",
      "['Shaker Resume_172', 'B Shaker-Sr BSA-Scrum Master _21', 'Krishna_BSA_72', 'Tarun RESUME-BSAT_194', 'Akhilprofile_6'] \n",
      "\n",
      "['Shail_Tank-Business Analyst_171', 'Bharatha BA Resume_31', 'BA - Abhishek_23', 'BA Kiran_25', 'Robinson_151']\n"
     ]
    }
   ],
   "source": [
    "ranked_result = RankResult(5, 'BM25Okapi')\n",
    "\n",
    "# Resume ranking given resume id\n",
    "print(ranked_result.get_ranking_with_resume_id(145),'\\n')\n",
    "\n",
    "# Resume ranking given raw query\n",
    "query = \"9+ years of experience in the field of business and data analysis supporting software solutions and analyzing business operations on various domains such as Banking,  Finance and Insurance. Worked in various software development environments including waterfall and agile methodologies including Scrum, Kanban, XP and SAFe. Implemented multiple projects in SOA architecture and dealt with APIs, SOAP and RESTful Web Services. Worked in the capacity of a certified Scrum Master by facilitating all scrum ceremonies, resolving Impediments and dependency issues. Proficient in data analytics – SQL querying, Ad Hoc / Canned report generation using tools like IBM Cognos BI and Tableau.\"\n",
    "print(ranked_result.get_ranking_with_query(query),'\\n')\n",
    "\n",
    "# Resume ranking given file_path and file_name\n",
    "file_name = 'Shail_Tank-Business Analyst.docx'\n",
    "path = \"./../Data/Resumes/\"\n",
    "print(ranked_result.get_ranking_with_resume_filename(path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
