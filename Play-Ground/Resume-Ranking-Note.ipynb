{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Ranking\n",
    "\n",
    "___ \n",
    "We try to rank resume profiles based on content similarity. The BM25 family of ranking models were considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textract\n",
    "# !pip install tika"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import textract\n",
    "from tika import parser\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths to inputs and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./../Data/Resumes/\"\n",
    "save_to_path = \"./../Data/Workin_Data/\"\n",
    "\n",
    "for filename in glob.glob(path+\"~*\"):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility class that serves as helper class containing functions regulary used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "\n",
    "\n",
    "class Utils:\n",
    "    \"\"\"\n",
    "    Class containing methods that serve as helper functions\n",
    "    \"\"\"\n",
    "    \n",
    "    def shuffle_data(self, data_pd):\n",
    "        \"\"\"\n",
    "        Data shuffling\n",
    "        \"\"\"\n",
    "        \n",
    "        data_columns = data_pd.columns\n",
    "        data_body = data_pd[data_columns]\n",
    "        data_body = shuffle(data_body)\n",
    "\n",
    "        return data_body\n",
    "    \n",
    "    def string_to_words(self, query):\n",
    "        \"\"\"\n",
    "        from string of words to list of processed words\n",
    "        \"\"\"\n",
    "        \n",
    "        nltk.download(\"stopwords\", quiet=True)\n",
    "        try:\n",
    "            # add_similar_words_to_search_query(query[-1])\n",
    "            text = BeautifulSoup(query[-1], \"html.parser\").get_text()  # Remove HTML tags\n",
    "            text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())  # Remove non-alphanumeric and Convert to lower case\n",
    "        except:\n",
    "            text = ''\n",
    "        word_list = text.split()  # Split string into words\n",
    "        word_list = [w for w in word_list if w not in stopwords.words(\"english\")]  # Remove stopwords\n",
    "        word_list = [PorterStemmer().stem(w) for w in word_list]  # stem\n",
    "\n",
    "        return [query[0], word_list]\n",
    "    \n",
    "    def clean_data(self, data, cache_dir, cache_file=\"cleaned_data.pkl\"):\n",
    "        \"\"\"\n",
    "        Convert each data row to words; read from cache if available.\n",
    "        input: dataframe with columns key->col1, value->col2\n",
    "        output: list of lists, e.g [[employee1_id,body1_word_list],[employee2_id,body2_word_list],...]\n",
    "        \"\"\"\n",
    "\n",
    "        data_keys, data_body = data[data.columns[0]].values, data[data.columns[1]].values\n",
    "        data_train = [[data_keys[i], data_body[i]] for i in range(len(data_body))]\n",
    "\n",
    "        # If cache_file is not None, try to read from it first\n",
    "        cache_data = None\n",
    "        if cache_file is not None:\n",
    "            try:\n",
    "                with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                    cache_data = pickle.load(f)\n",
    "                print(\"Read cleaned data from cache file:\", cache_file)\n",
    "            except:\n",
    "                pass  # unable to read from cache, but that's okay\n",
    "\n",
    "        # If cache is missing, then do the heavy lifting\n",
    "        if cache_data is None:\n",
    "            # Preprocess the data to obtain words for each employee data\n",
    "            words_train = list(map(self.string_to_words, data_train))\n",
    "\n",
    "            # Write to cache file for future runs\n",
    "            if cache_file is not None:\n",
    "                cache_data = dict(words_train=words_train)\n",
    "                with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                    pickle.dump(cache_data, f)\n",
    "                print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "        else:\n",
    "            # Unpack data loaded from cache file\n",
    "            words_train = (cache_data['words_train'])\n",
    "\n",
    "        return words_train\n",
    "    \n",
    "    def add_data_to_pickle(self, data_file, data=None, path='./'):\n",
    "\n",
    "        data_path = path + data_file\n",
    "\n",
    "        data_file_name = data_file.split('.')[0]\n",
    "        pickle_file_name = path + 'data_dict.pkl'\n",
    "\n",
    "        if os.path.isfile(pickle_file_name):\n",
    "            pickle_file = open(pickle_file_name, 'rb')\n",
    "        else:\n",
    "            pickle_file = open(pickle_file_name, 'bw')\n",
    "            pickle_file.close()\n",
    "\n",
    "        if os.path.getsize(pickle_file_name) > 0:\n",
    "            data_collections = pickle.load(pickle_file)\n",
    "            pickle_file.close()\n",
    "        else:\n",
    "            data_collections = {}\n",
    "\n",
    "\n",
    "        data_collections[data_file_name] = data\n",
    "        with open(pickle_file_name, 'bw') as f:\n",
    "            pickle.dump(data_collections, f)\n",
    "            \n",
    "    def swap_key_value(self, index_id):\n",
    "        return {emm_id: index for index, emm_id in index_id.items()}\n",
    "    \n",
    "    def resume_index_id_data(self, data_processed_with_id):\n",
    "        count, index_id, data = 0, {}, []\n",
    "        for item in data_processed_with_id:\n",
    "            index_id[count] = item[0]\n",
    "            data.append(item[1])\n",
    "            count += 1\n",
    "        return index_id, data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class containing functions that helps to extract data from the document resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecruitmentPreprocess:\n",
    "    \"\"\"\n",
    "    Class for preprocessing job offer and resume text queries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resume_path, save_to_path=None):\n",
    "        \n",
    "        self.resume_path = resume_path\n",
    "        \n",
    "        if save_to_path != None:\n",
    "            self.save_to_path = save_to_path\n",
    "            self.resume_id_index, self.resume_id_data, self.resume_data = self.__merge_resume_to_dataframe()\n",
    "        \n",
    "\n",
    "    def extract_text_from_resume(self, file_name):\n",
    "        if file_name.split('.')[-1] == \"pdf\":\n",
    "            text = parser.from_file(self.resume_path + file_name)['content']\n",
    "        else:\n",
    "            text = textract.process(self.resume_path + file_name).decode()\n",
    "        return text\n",
    "\n",
    "\n",
    "    def __merge_resume_to_dataframe(self):\n",
    "\n",
    "        all_collection = []\n",
    "        all_files = [file.split('\\\\')[-1] for file in glob.glob(path + \"*\") if not file.startswith('~')]\n",
    "        index = 1\n",
    "        resume_id_index = {}\n",
    "        resume_id_data = {}\n",
    "        \n",
    "        for file in all_files:\n",
    "            if file.split('.')[-1] in ['docx', 'pdf', 'doc']:\n",
    "                resume_id_index[index] = '{}_{}'.format(''.join(file.split('.')[:-1]),index)\n",
    "                resume_id_data[index] = self.extract_text_from_resume(file)\n",
    "                collection = [index, self.extract_text_from_resume(file)]\n",
    "                all_collection.append(collection)\n",
    "                index += 1\n",
    "\n",
    "        return resume_id_index, resume_id_data, pd.DataFrame(all_collection, columns=['employee_id', 'data'])\n",
    "    \n",
    "    def save_resume_data_to_csv(self):\n",
    "        \n",
    "        self.resume_data.to_csv(self.save_to_path + 'resume_data.csv', index=False)\n",
    "        \n",
    "    def add_resume_keys_to_pickle(self):\n",
    "        \n",
    "        utils = Utils()\n",
    "        utils.add_data_to_pickle('resume_id_index', self.resume_id_index, self.save_to_path)\n",
    "        \n",
    "    def add_user_accessible_resume_to_pickle(self):\n",
    "        \n",
    "        utils = Utils()\n",
    "        utils.add_data_to_pickle('user_accessible_resume', self.resume_id_data, self.save_to_path)\n",
    "        \n",
    "    def add_processed_resume_to_pickle(self):\n",
    "        \n",
    "        cache_directory = os.path.join(\"cache\", \"words_tokens\")  # where to store cache files\n",
    "        os.makedirs(cache_directory, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "        cache_file = 'cleaned_{}.pkl'.format('processed_resume')\n",
    "        \n",
    "        utils = Utils()\n",
    "    \n",
    "        data_shuffled = utils.shuffle_data(self.resume_data)\n",
    "        data_processed_with_id = utils.clean_data(data_shuffled, cache_directory, cache_file=cache_file)\n",
    "        \n",
    "        shutil.rmtree('cache')\n",
    "        \n",
    "        utils.add_data_to_pickle('processed_resume', data_processed_with_id, self.save_to_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the resume files and storing them in a pickle file for faster access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: cleaned_processed_resume.pkl\n"
     ]
    }
   ],
   "source": [
    "resume = RecruitmentPreprocess(path, save_to_path)\n",
    "resume.save_resume_data_to_csv()\n",
    "resume.add_resume_keys_to_pickle()\n",
    "resume.add_user_accessible_resume_to_pickle()\n",
    "resume.add_processed_resume_to_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming that the files were extracted successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['resume_id_index', 'user_accessible_resume', 'processed_resume', 'resume_count', 'resume_data'])\n",
      "     employee_id                                               data\n",
      "0              1  Name: Abiral Pandey\\n\\nEmail: abiral.pandey88@...\n",
      "1              2  Achyuth\\n\\n540-999-8048\\n\\nachyuth.java88@gmai...\n",
      "2              3  Adelina Erimia, PMP, Six Sigma Green Belt, SMC...\n",
      "3              4  Adhi Gopalam\\n\\nadhigopalam@gmail.com\\n\\n281-2...\n",
      "4              5  Ajay Kumar (CSM)\\t     \\t\\t     Email/Skype: a...\n",
      "..           ...                                                ...\n",
      "219          220  VISHNU J\\n\\nEmail to jammigumpulavishnu452@gma...\n",
      "220          221  Cell: 972-514-3667\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nVivek Jo...\n",
      "221          222  VIVEK SAGAR                                   ...\n",
      "222          223  YOHAN \\n\\nSr. Business Analyst\\n\\n\\n\\nVersatil...\n",
      "223          224  Yugesh\\n\\n+1(515)-650-2459\\n\\nyugeshm4@gmail.c...\n",
      "\n",
      "[224 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(save_to_path+'data_dict.pkl', 'rb') as f:\n",
    "    cv_id_index = pickle.load(f)\n",
    "\n",
    "print(cv_id_index.keys())\n",
    "print(cv_id_index['resume_data'])\n",
    "# print(cv_id_index['user_accessible_resume'][144])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The family of BM25 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, corpus, tokenizer=None):\n",
    "        self.corpus_size = len(corpus)\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if tokenizer:\n",
    "            corpus = self._tokenize_corpus(corpus)\n",
    "\n",
    "        nd = self._initialize(corpus)\n",
    "        self._calc_idf(nd)\n",
    "\n",
    "    def _initialize(self, corpus):\n",
    "        nd = {}  # word -> number of documents with word\n",
    "        num_doc = 0\n",
    "        for document in corpus:\n",
    "            self.doc_len.append(len(document))\n",
    "            num_doc += len(document)\n",
    "\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.doc_freqs.append(frequencies)\n",
    "\n",
    "            for word, freq in frequencies.items():\n",
    "                try:\n",
    "                    nd[word] += 1\n",
    "                except KeyError:\n",
    "                    nd[word] = 1\n",
    "\n",
    "        self.avgdl = num_doc / self.corpus_size\n",
    "        return nd\n",
    "\n",
    "    def _tokenize_corpus(self, corpus):\n",
    "        pool = Pool(cpu_count())\n",
    "        tokenized_corpus = pool.map(self.tokenizer, corpus)\n",
    "        return tokenized_corpus\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_top_n(self, query, documents, n=5):\n",
    "\n",
    "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
    "\n",
    "        scores = self.get_scores(query)\n",
    "        top_n = np.argsort(scores)[::-1][:n]\n",
    "        return [documents[i] for i in top_n]\n",
    "\n",
    "\n",
    "class BM25Okapi(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
    "        # k1=1.5, b=0.75, epsilon=0.25 -> k1=0 to 3, b -> 0 to 1\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        \"\"\"\n",
    "        Calculates frequencies of terms in documents and in corpus.\n",
    "        This algorithm sets a floor on the idf values to eps * average_idf\n",
    "        \"\"\"\n",
    "        # collect idf sum to calculate an average idf for epsilon value\n",
    "        idf_sum = 0\n",
    "        # collect words with negative idf to set them a special epsilon value.\n",
    "        # idf can be negative if word is contained in more than half of documents\n",
    "        negative_idfs = []\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                negative_idfs.append(word)\n",
    "        self.average_idf = idf_sum / len(self.idf)\n",
    "\n",
    "        eps = self.epsilon * self.average_idf\n",
    "        for word in negative_idfs:\n",
    "            self.idf[word] = eps\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        \"\"\"\n",
    "        The ATIRE BM25 variant uses an idf function which uses a log(idf) score. To prevent negative idf scores,\n",
    "        this algorithm also adds a floor to the idf value of epsilon.\n",
    "        See [Trotman, A., X. Jia, M. Crane, Towards an Efficient and Effective Search Engine] for more info\n",
    "        :param query:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
    "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
    "        return score\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        \"\"\"\n",
    "        Calculate bm25 scores between query and subset of all docs\n",
    "        \"\"\"\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "        score = np.zeros(len(doc_ids))\n",
    "        doc_len = np.array(self.doc_len)[doc_ids]\n",
    "        for q in query:\n",
    "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
    "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
    "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
    "        return score.tolist()\n",
    "\n",
    "\n",
    "class BM25L(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log(self.corpus_size + 1) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
    "            score += (self.idf.get(q) or 0) * q_freq * (self.k1 + 1) * (ctd + self.delta) / \\\n",
    "                     (self.k1 + ctd + self.delta)\n",
    "        return score\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        \"\"\"\n",
    "        Calculate bm25 scores between query and subset of all docs\n",
    "        \"\"\"\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "        score = np.zeros(len(doc_ids))\n",
    "        doc_len = np.array(self.doc_len)[doc_ids]\n",
    "        for q in query:\n",
    "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
    "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
    "            score += (self.idf.get(q) or 0) * q_freq * (self.k1 + 1) * (ctd + self.delta) / \\\n",
    "                     (self.k1 + ctd + self.delta)\n",
    "        return score.tolist()\n",
    "\n",
    "\n",
    "class BM25Plus(BM25):\n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus, tokenizer)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log((self.corpus_size + 1) / freq)\n",
    "            self.idf[word] = idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
    "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
    "        return score\n",
    "\n",
    "    def get_batch_scores(self, query, doc_ids):\n",
    "        \"\"\"\n",
    "        Calculate bm25 scores between query and subset of all docs\n",
    "        \"\"\"\n",
    "        assert all(di < len(self.doc_freqs) for di in doc_ids)\n",
    "        score = np.zeros(len(doc_ids))\n",
    "        doc_len = np.array(self.doc_len)[doc_ids]\n",
    "        for q in query:\n",
    "            q_freq = np.array([(self.doc_freqs[di].get(q) or 0) for di in doc_ids])\n",
    "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
    "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
    "        return score.tolist()\n",
    "\n",
    "\n",
    "# BM25Adpt and BM25T are a bit more complicated than the previous algorithms here. Here a term-specific k1\n",
    "# parameter is calculated before scoring is done\n",
    "\n",
    "class BM25Adpt(BM25):\n",
    "    def __init__(self, corpus, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log((self.corpus_size + 1) / freq)\n",
    "            self.idf[word] = idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
    "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
    "        return score\n",
    "\n",
    "\n",
    "\n",
    "class BM25T(BM25):\n",
    "    def __init__(self, corpus, k1=1.5, b=0.75, delta=1):\n",
    "        # Algorithm specific parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.delta = delta\n",
    "        super().__init__(corpus)\n",
    "\n",
    "    def _calc_idf(self, nd):\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log((self.corpus_size + 1) / freq)\n",
    "            self.idf[word] = idf\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
    "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that helps to get the ranking scores from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "class ModelScores:\n",
    "    \"\"\"\n",
    "    Returns Model Scores for single query, single resume and for all resumes \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        with open('./../Data/Workin_Data/'+'data_dict.pkl', 'rb') as f:\n",
    "            self.pickle_data = pickle.load(f)\n",
    "            \n",
    "        self.utils = Utils()\n",
    "    \n",
    "    def get_resume_id_ranking_scores(self, resume_id, model):\n",
    "        \n",
    "        user_accessible_resume = self.pickle_data['user_accessible_resume']\n",
    "        processed_resume = self.pickle_data['processed_resume']\n",
    "\n",
    "        data_index_resume_id, data_processed = self.utils.resume_index_id_data(processed_resume)\n",
    "\n",
    "        # Remove the resume from the list of resumes to be compared to\n",
    "        data_processed_copy = data_processed[:]\n",
    "        index = self.utils.swap_key_value(data_index_resume_id)[resume_id]\n",
    "        del data_processed_copy[index]\n",
    "\n",
    "        raw_query = user_accessible_resume[resume_id]\n",
    "\n",
    "        if model == 'BM25':\n",
    "            chosen_model = BM25(data_processed_copy)\n",
    "        elif model == 'BM25Okapi':\n",
    "            chosen_model = BM25Okapi(data_processed_copy)\n",
    "        elif model == 'BM25L':\n",
    "            chosen_model = BM25L(data_processed_copy)\n",
    "        elif model == 'BM25Adpt':\n",
    "            chosen_model = BM25Adpt(data_processed_copy)\n",
    "        elif model == 'BM25T':\n",
    "            chosen_model = BM25T(data_processed_copy)\n",
    "        else:\n",
    "            chosen_model = BM25Plus(data_processed_copy)\n",
    "\n",
    "        scores = list(chosen_model.get_scores(self.utils.string_to_words(['', raw_query])[-1]))\n",
    "\n",
    "        scores.insert(index, float('inf'))\n",
    "        indices, id_sorted = zip(*sorted(enumerate(scores), reverse=True, key=itemgetter(1)))\n",
    "\n",
    "        return [[data_index_resume_id[indices[i]], id_sorted[i]] for i in range(0, len(indices))][1:]\n",
    "    \n",
    "\n",
    "    def single_query_scores(self, query, model):\n",
    "        \n",
    "        processed_resume = self.pickle_data['processed_resume']\n",
    "        data_index_employee_id, data_processed = self.utils.resume_index_id_data(processed_resume)\n",
    "        \n",
    "        if model == 'BM25':\n",
    "            chosen_model = BM25(data_processed)\n",
    "        elif model == 'BM25Okapi':\n",
    "            chosen_model = BM25Okapi(data_processed)\n",
    "        elif model == 'BM25L':\n",
    "            chosen_model = BM25L(data_processed)\n",
    "        elif model == 'BM25Adpt':\n",
    "            chosen_model = BM25Adpt(data_processed)\n",
    "        elif model == 'BM25T':\n",
    "            chosen_model = BM25T(data_processed)\n",
    "        else:\n",
    "            chosen_model = BM25Plus(data_processed)\n",
    "            \n",
    "        scores = list(chosen_model.get_scores(self.utils.string_to_words(['', query])[-1]))\n",
    "        indices, id_sorted = zip(*sorted(enumerate(scores), reverse=True, key=itemgetter(1)))\n",
    "        \n",
    "        return [[data_index_employee_id[indices[i]], id_sorted[i]] for i in range(len(indices))]\n",
    "    \n",
    "    def single_resume_scores(self, path, file_name, model):\n",
    "        \n",
    "        recruitment_preprocess = RecruitmentPreprocess(path)\n",
    "        query = recruitment_preprocess.extract_text_from_resume(file_name)\n",
    "        \n",
    "        return self.single_query_scores(file_name, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class that helps to extract the resume names from the ranked ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class RankResult:\n",
    "    \n",
    "    def __init__(self, no_of_output, model):\n",
    "        \n",
    "        with open(save_to_path+'data_dict.pkl', 'rb') as f:\n",
    "            pickle_data = pickle.load(f)\n",
    "        self.user_accessible_resume = pickle_data['resume_id_index']\n",
    "        self.no_of_output = no_of_output\n",
    "        self.model_scores = ModelScores()\n",
    "        self.model = model\n",
    "        \n",
    "    def get_ranking_with_resume_id(self, resume_id):\n",
    "        \n",
    "        scores = self.model_scores.get_resume_id_ranking_scores(145, self.model)\n",
    "        ranked_Resume_names = []\n",
    "        \n",
    "        for i in range(self.no_of_output):\n",
    "            if i >= len(scores):\n",
    "                break\n",
    "            ranked_Resume_names.append(self.user_accessible_resume[scores[i][0]])\n",
    "        \n",
    "        return ranked_Resume_names\n",
    "    \n",
    "    def get_ranking_with_query(self, query):\n",
    "        \n",
    "        scores = self.model_scores.single_query_scores(query, self.model)\n",
    "        ranked_Resume_names = []\n",
    "        \n",
    "        for i in range(self.no_of_output):\n",
    "            if i >= len(scores):\n",
    "                break\n",
    "            ranked_Resume_names.append(self.user_accessible_resume[scores[i][0]])\n",
    "        \n",
    "        return ranked_Resume_names\n",
    "    \n",
    "    def get_ranking_with_resume_filename(self, path, file_name):\n",
    "        \n",
    "        scores = self.model_scores.single_resume_scores(path, file_name, self.model)\n",
    "        ranked_Resume_names = []\n",
    "        \n",
    "        for i in range(self.no_of_output):\n",
    "            if i >= len(scores):\n",
    "                break\n",
    "            ranked_Resume_names.append(self.user_accessible_resume[scores[i][0]])\n",
    "        \n",
    "        return ranked_Resume_names\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "\n",
    "1. Get the resume ranking given resume id\n",
    "2. Get the resume ranking given raw query\n",
    "3. Get the resume ranking given file path and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUNITHA Project Manager (1)_191', 'Srivatsan_Project_Manager_187', 'Sahas BA Resume_154', 'AjayKumar_5', 'Syed_Zia_Ashraf_192'] \n",
      "\n",
      "['Shaker Resume_172', 'B Shaker-Sr BSA-Scrum Master _21', 'Krishna_BSA_72', 'Tarun RESUME-BSAT_194', 'Akhilprofile_6'] \n",
      "\n",
      "['Shail_Tank-Business Analyst_171', 'Bharatha BA Resume_31', 'BA - Abhishek_23', 'BA Kiran_25', 'Robinson_151']\n"
     ]
    }
   ],
   "source": [
    "ranked_result = RankResult(5, 'BM25Okapi')\n",
    "\n",
    "# Resume ranking given resume id\n",
    "print(ranked_result.get_ranking_with_resume_id(145),'\\n')\n",
    "\n",
    "# Resume ranking given raw query\n",
    "query = \"9+ years of experience in the field of business and data analysis supporting software solutions and analyzing business operations on various domains such as Banking,  Finance and Insurance. Worked in various software development environments including waterfall and agile methodologies including Scrum, Kanban, XP and SAFe. Implemented multiple projects in SOA architecture and dealt with APIs, SOAP and RESTful Web Services. Worked in the capacity of a certified Scrum Master by facilitating all scrum ceremonies, resolving Impediments and dependency issues. Proficient in data analytics â€“ SQL querying, Ad Hoc / Canned report generation using tools like IBM Cognos BI and Tableau.\"\n",
    "print(ranked_result.get_ranking_with_query(query),'\\n')\n",
    "\n",
    "# Resume ranking given file_path and file_name\n",
    "file_name = 'Shail_Tank-Business Analyst.docx'\n",
    "path = \"./../Data/Resumes/\"\n",
    "print(ranked_result.get_ranking_with_resume_filename(path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
